---
title: slurm分布式调度系统搭建
date: 2017-07-21 16:07:30
tags: []
categories: [分布式]
---
### 创建云主机集群
登录清华大学[EasyStack](http://cloud.iiis.systems/)，概况中可以看到目前集群资源的使用情况，点击左侧计算资源-云主机，选择创建云主机对云主机进行创建。填入云主机名字，例如canu，选择从镜像中安装云主机，镜像选择CentOS6.5，之后对配置进行设置，对于canu项目来说建议选择16core-64GB内存-500GB磁盘的配置，网络选择share_net，安全组中将joey安全策略加入，密码设置自己密码即可，之后点击创建云主机。由于本次需要搭建一个集群，我们先完整配置好一台机器再通过该机器建立云主机快照，根据该快照来创建其他节点（即将这台机器一模一样复制多台出来）。主机创建完毕大概需要20分钟左右。

创建完毕之后需要对该主机绑定公网ip，公网ip可以到网络资源中进行申请，选择100Mbps的公网。选中之前创建的那台云主机，在更多种选择绑定公网ip，将之前申请的公网ip绑定上去。之后便可以通过vpn来连接该主机，由于windows连接清华大学vpn比较麻烦，已经在gpu-server5上打开了与清华大学的vpn连接，可以首先连接gpu-server5再连接清华大学集群。

### 搭建slurm环境
1. 刚创建的云主机只有root用户，一般用root用户操作有许多不便之处，需要先创建slurm用户来进行操作。(#表示需要sudo权限或root用户的操作)

```
# useradd slurm
```
之后需要增加slurm用户的sudo权限

```
# chmod -v u+w /etc/sudoers
# yum install vim git -y
# vim /etc/sudoers
```
模仿root用户的权限控制加入`slurm	ALL=(ALL)	ALL`
之后需要恢复`/etc/sudoers`的权限，使用`# chmod -v u-w /etc/sudoers`

接下来`su slurm`进行操作
<!--more-->
2. 编译安装munge，slurm通过munge来进行通信，虽然可以直接使用yum来安装munge，但这样安装的munge很容易出问题，使用编译安装的方式进行安装。下载[编译安装包](https://github.com/dun/munge/releases/download/munge-0.5.11/munge-0.5.11.tar.bz2)。编译安装之前需要先安装一些编译所依赖的包（可能没有列完全，需要什么依赖会有提示，按照需要的依赖自行安装即可）

后来发现编译安装的munge不会有create-munge-key这个命令，还是需要用yum来进行安装，`yum install munge munge-devel`

```
# yum install -y rpm-build rpmdevtools bzip2-devel openssl-devel zlib-devel
```
之后使用如下方法进行编译安装

```
# rpmbuild -tb --clean munge-0.5.11.tar.bz2
# cd /root/rpmbuild/RPMS/x86_64
# rpm --install munge*.rpm
```

之后需要创建文件夹并修改所有者（如果文件夹已经存在则不需要创建）

```
# mkdir /etc/munge
# mkdir /var/run/munge
# mkdir /var/lib/munge
# mkdir /var/log/munge
# chown slurm:slurm /etc/munge
# chown slurm:slurm /var/run/munge
# chown slurm:slurm /var/lib/munge
# chown slurm:slurm /var/log/munge
```

使用`create-munge-key`命令来创建秘钥，并修改所有者`# chown slurm:slurm /etc/munge/munge.key`

启动munge的命令为`munged`，可以使用`ps aux | grep munged`来判断munge是否正常启动

3. 编译安装slurm，下载[编译安装包15.08.13](https://www.schedmd.com/archives.php)，首先需要安装依赖

```
# yum install -y readline-devel pam-devel perl-DBI perl-ExtUtils-MakeMaker
```
之后使用如下方法进行编译安装

```
# rpmbuild -ta --clean slurm-15.08.13.tar.bz2
# cd /root/rpmbuild/RPMS/x86_64
# rpm --install slurm*.rpm
```
修改slurm.conf文件

```
# cp /etc/slurm/slurm.conf.example /etc/slurm.conf
# vim /etc/slurm/slurm.conf
```
参考配置
```
#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=canu
ControlMachine=canu
ControlAddr=192.168.73.202
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/tmp
SlurmdSpoolDir=/tmp/slurmd
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
#PluginDir=
#FirstJobId=
ReturnToService=0
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
#SelectType=select/linear
FastSchedule=1
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
#JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherFrequency=30
#
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=canu
#AccountingStorageLoc=
AccountingStoragePass=elwg324
AccountingStorageUser=root
#
# COMPUTE NODES
NodeName=canu NodeAddr=192.168.73.202 CPUs=12 RealMemory=64000 State=UNKNOWN
NodeName=slurm-1 NodeAddr=192.168.73.203 CPUs=12 RealMemory=64000 State=UNKNOWN
NodeName=slurm-2 NodeAddr=192.168.73.204 CPUs=12 RealMemory=64000 State=UNKNOWN
NodeName=slurm-3 NodeAddr=192.168.73.205 CPUs=12 RealMemory=64000 State=UNKNOWN
PartitionName=control Nodes=canu Default=NO MaxTime=INFINITE State=UP
PartitionName=compute Nodes=slurm-[1-3] Default=YES MaxTime=INFINITE State=UP
```

配置中关于节点的ip地址信息可以在清华集群控制台查看，slurm-1等节点现在还没有创建出来可以先不写。注意：此处需要配置ACCOUNTING，这个我之前没有配置，我这样配置不一定能够正常运行，你们需要去学习一下如何配置slurm的accounting，在slurm的官网当中有说明
https://slurm.schedmd.com/accounting.html

配置这个的好处是可以把每次任务的运行细节存储到数据库中，之后可以使用sacct命令来查看历史运行记录（运行时间等信息），没配置的话sacct无法使用。

### 将创建好的canu机器进行复制称为slave节点
在清华大学集群控制台计算资源-云主机中选择要创建镜像的云主机，在更多种选择创建快照，创建完成后创建slave节点，点击创建云主机，其他步骤都和之前所说相同，但这次不是从镜像安装机器，而是从云主机快照中选择刚刚创建的那个云主机快照，云主机名字写slurm数量可以根据需要写，会自动加入-1，-2，-3的后缀在slurm后面。

slave主机创建完成后，（暂且称第一台创建的主机为头节点，后面复制的节点为计算节点），将头节点和计算节点中/etc/hosts文件的内容进行添加，把每台机器的hostname和内网ip信息都加入进去。（可以先写好一台机器，然后scp到其他所有机器上去）

### 注意
1. 每计算节点创建完毕之后，都需要进入之后打开munge
2. 每个节点的slurm配置文件都必须完全一样
3. 节点打开slurm的方法为`# /etc/init.d/slurm start`
4. sinfo始终显示partition为down的状态可以先使用`# /etc/init.d/slurm stop`停止，再使用`# /etc/init.d/slurm cleanstart`来启动节点
5. munge运行能否成功和各种文件夹的权限所有者都很有关系，要仔细配置

### 参考网站
1. http://ju.outofmemory.cn/entry/203355
2. https://www.slothparadise.com/how-to-install-slurm-on-centos-7-cluster/
3. http://blog.csdn.net/datuqiqi/article/details/50827040

此外，我搭好的测试集群可以使用`ssh slurm@10.2.1.25`来进行查看

### 步骤
```
# useradd slurm
# passwd slurm
# yum install -y vim git ansible gcc gcc-c++ rpm-build rpmdevtools bzip2-devel openssl-devel zlib-devel readline-devel pam-devel perl-DBI perl-ExtUtils-MakeMaker munge munge-devel
# chmod -v u+w /etc/sudoers
# vim /etc/sudoers
在root	ALL=(ALL)	ALL 之后添加如下内容
slurm	ALL=(ALL)	ALL

# chmod -v u-w /etc/sudoers
# ssh-keygen
# ssh-copy-id canu
# su slurm
$ ssh-keygen
$ ssh-copy-id canu
$ sudo mkdir /etc/munge
$ sudo mkdir /var/run/munge
$ sudo mkdir /var/lib/munge
$ sudo mkdir /var/log/munge
$ sudo chown slurm:slurm /etc/munge
$ sudo chown slurm:slurm /var/run/munge
$ sudo chown slurm:slurm /var/lib/munge
$ sudo chown slurm:slurm /var/log/munge
$ sudo create-munge-key
$ sudo chown slurm:slurm /etc/munge/munge.key
此时先不要启动munge

$ rpmbuild -ta --clean slurm-15.08.13.tar.bz2
$ cd /root/rpmbuild/RPMS/x86_64
$ sudo rpm --install slurm*.rpm
$ sudo cp /etc/slurm/slurm.conf.example /etc/slurm/slurm.conf
$ sudo vim /etc/slurm/slurm.conf
修改slurm的配置文件，有关slave的IP信息先放着不填
```

制作快照，并建立其他slave节点

```
$ sudo vim /etc/hosts
将master节点和slave节点的ip信息写入，格式为xxx.xxx.xxx.xxx	hostname
并将hosts文件拷贝到所有slave节点当中去

$ sudo vim /etc/ansible/hosts
写入两个控制group，一个包含有master节点，一个包含有slave节点，具体写法参照10.2.0.80中的写法
接下来需要关闭每个节点的防火墙
$ sudo ansible slurm-all -m shell -a 'service iptables stop'
$ ansible slurm-all -m shell -a '/etc/init.d/munge start'
$ sudo ansible slurm-all -m shell -a '/etc/init.d/slurm start'
```
启动munge不要用sudo，启动slurm需要使用sudo权限
